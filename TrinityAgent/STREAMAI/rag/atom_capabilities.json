{
  "atoms": [
    {
      "atom_id": "merge",
      "name": "Merge/Join Datasets",
      "description": "Combines two datasets based on common columns (like SQL JOIN)",
      "endpoint": "/trinityai/merge",
      "backend_endpoint": "/api/merge/perform",
      "capabilities": [
        "Join two datasets on common columns",
        "Inner join, outer join, left join, right join",
        "Match rows based on key columns",
        "Combine datasets with related data"
      ],
      "use_cases": [
        "User says: merge, join, combine, link, match, inner join, outer join",
        "Combining sales data with customer data",
        "Linking product data with inventory data",
        "Matching records by ID or key columns"
      ],
      "required_parameters": {
        "file1": "Full path to first file (e.g., 'prefix/filename.arrow')",
        "file2": "Full path to second file (e.g., 'prefix/filename.arrow')",
        "join_columns": "Comma-separated list of columns to join on (e.g., 'CustomerID,OrderID')",
        "join_type": "Type of join: 'inner', 'outer', 'left', 'right'"
      },
      "optional_parameters": {
        "bucket_name": "MinIO bucket name (usually 'trinity')"
      },
      "prompt_requirements": [
        "MUST include exact file paths/names from file details",
        "MUST specify which columns to join on (use exact column names)",
        "MUST specify join type (inner/outer/left/right)",
        "Example: 'Merge D0_KHC_UK_Beans.arrow and D0_KHC_UK_Mayo.arrow on columns Year and Region using inner join'"
      ],
      "output": "Merged dataset saved as new Arrow file",
      "typical_next_atoms": ["correlation", "feature-overview", "groupby-wtg-avg", "dataframe-operations", "chart-maker"]
    },
    {
      "atom_id": "concat",
      "name": "Concatenate Datasets",
      "description": "Stacks multiple datasets vertically (append rows) or horizontally (append columns)",
      "endpoint": "/trinityai/concat",
      "backend_endpoint": "/api/concat/perform",
      "capabilities": [
        "Append rows from multiple datasets",
        "Stack datasets vertically",
        "Combine datasets without key matching",
        "Append columns horizontally (optional)"
      ],
      "use_cases": [
        "User says: concat, concatenate, append, stack, combine vertically",
        "Combining quarterly data files",
        "Appending new data to existing dataset",
        "Stacking similar datasets together"
      ],
      "required_parameters": {
        "file1": "Full path to first file (e.g., 'prefix/filename.arrow')",
        "file2": "Full path to second file (e.g., 'prefix/filename.arrow')",
        "concat_direction": "Direction: 'vertical' (append rows) or 'horizontal' (append columns)"
      },
      "optional_parameters": {},
      "prompt_requirements": [
        "MUST include exact file paths/names from file details",
        "MUST specify direction: 'vertical' (rows) or 'horizontal' (columns)",
        "Example: 'Concatenate D0_KHC_UK_Beans.arrow and D0_KHC_UK_Mayo.arrow vertically'"
      ],
      "output": "Concatenated dataset saved as new Arrow file",
      "typical_next_atoms": ["correlation", "feature-overview", "groupby-wtg-avg", "dataframe-operations", "chart-maker"]
    },
    {
      "atom_id": "groupby-wtg-avg",
      "name": "Group and Aggregate",
      "description": "Groups data by columns and calculates aggregations (sum, mean, count, etc.)",
      "endpoint": "/trinityai/groupby",
      "backend_endpoint": "/api/groupby-weight-avg/perform",
      "capabilities": [
        "Group data by one or more columns",
        "Calculate aggregations: sum, mean, average, count, min, max",
        "Weighted averages",
        "Multiple aggregation functions per group"
      ],
      "use_cases": [
        "User says: group by, aggregate, sum, average, mean, total, count, summarize",
        "Group sales by region and calculate total revenue",
        "Calculate average price by product category",
        "Count records by status or category"
      ],
      "required_parameters": {
        "data_source": "File path or result from previous step",
        "group_columns": "Array of column names to group by (use EXACT names from file details)",
        "aggregations": "Array of aggregation objects with measure columns and functions"
      },
      "optional_parameters": {
        "weight_column": "Column name for weighted average calculation"
      },
      "prompt_requirements": [
        "MUST use exact column names from file details (case-sensitive)",
        "MUST specify which columns to group by",
        "MUST specify what to aggregate (sum, mean, count, etc.)",
        "MUST specify which columns to aggregate",
        "Example: 'Group D0_KHC_UK_Beans.arrow by Region column and calculate sum of Revenue column'"
      ],
      "output": "Grouped and aggregated dataset",
      "typical_next_atoms": ["correlation", "chart-maker", "dataframe-operations", "feature-overview"]
    },
    {
      "atom_id": "dataframe-operations",
      "name": "DataFrame Operations",
      "description": "Comprehensive Excel-like DataFrame manipulation: formulas, filters, sorts, transformations",
      "endpoint": "/trinityai/dataframe-operations",
      "backend_endpoint": "/api/dataframe-operations",
      "capabilities": [
        "Apply formulas/calculations (PROD, SUM, DIV, IF, AVG, MAX, MIN, etc.)",
        "Filter rows based on conditions",
        "Sort data by columns",
        "Select/drop/rename columns",
        "Transform data (case conversion, type conversion, rounding)",
        "Insert/delete rows or columns",
        "Edit cell values",
        "Find and replace values",
        "Excel-like spreadsheet operations"
      ],
      "use_cases": [
        "User says: filter, where, sort, order, formula, calculate, compute, transform, convert, round, edit, insert, delete, find, replace, excel, spreadsheet, manipulate, clean, prepare",
        "Filter rows where Revenue > 1000",
        "Apply formula: PROD(Price, Quantity) to calculate Sales",
        "Sort data by Date column descending",
        "Rename column 'A' to 'Revenue'",
        "Convert text to uppercase",
        "Round numbers to 2 decimal places"
      ],
      "required_parameters": {
        "prompt": "Detailed natural language instruction with exact column names and values",
        "df_id": "DataFrame ID from previous step (or 'auto_from_previous')"
      },
      "optional_parameters": {
        "object_name": "File path if loading new file (must end with .arrow)"
      },
      "prompt_requirements": [
        "CRITICAL: Prompt MUST include exact column names from file details (case-sensitive, with spaces)",
        "CRITICAL: Prompt MUST include specific values/conditions (not generic)",
        "CRITICAL: Prompt MUST reference actual data types (numeric vs categorical)",
        "CRITICAL: Use unique values from categorical columns for filters",
        "Example: 'Filter rows where Region = \"North\" AND Revenue > 1000, then sort by Date descending'",
        "Example: 'Apply formula PROD(Price, Quantity) to create Sales column'",
        "Example: 'Rename column \"Product Name\" to \"Product\"'",
        "BAD Example: 'Filter data' (too generic)",
        "GOOD Example: 'Filter D0_KHC_UK_Beans.arrow where Year = 2023 AND Country = \"UK\", then sort by Revenue descending'"
      ],
      "output": "Transformed DataFrame with operations applied",
      "typical_next_atoms": ["correlation", "chart-maker", "groupby-wtg-avg", "feature-overview", "dataframe-operations"]
    },
    {
      "atom_id": "create-column",
      "name": "Create Calculated Column",
      "description": "Creates new columns using formulas and calculations",
      "endpoint": "/trinityai/create-transform",
      "backend_endpoint": "/api/create-column/perform",
      "capabilities": [
        "Create new calculated columns",
        "Apply formulas and transformations",
        "Derive columns from existing data"
      ],
      "use_cases": [
        "User says: create column, add column, calculate, compute, derive, new column",
        "Create profit column as revenue minus cost",
        "Calculate percentage columns",
        "Derive computed metrics"
      ],
      "required_parameters": {
        "data_source": "File path or result from previous step",
        "new_column_name": "Name for the new column",
        "formula": "Formula or calculation expression"
      },
      "optional_parameters": {},
      "prompt_requirements": [
        "MUST specify exact column names used in formula",
        "MUST specify the new column name",
        "MUST include the formula/calculation",
        "Example: 'Create Profit column as Revenue minus Cost in D0_KHC_UK_Beans.arrow'"
      ],
      "output": "Dataset with new calculated column",
      "typical_next_atoms": ["dataframe-operations", "chart-maker", "groupby-wtg-avg"]
    },
    {
      "atom_id": "chart-maker",
      "name": "Create Visualizations",
      "description": "Generates charts and visualizations (bar, line, pie, scatter, etc.)",
      "endpoint": "/trinityai/chart-maker",
      "backend_endpoint": "/api/chart-maker/generate",
      "capabilities": [
        "Create bar charts, line charts, pie charts, scatter plots",
        "Visualize data relationships",
        "Interactive charts",
        "Multiple chart types"
      ],
      "use_cases": [
        "User says: chart, plot, graph, visualize, show, display, bar chart, line chart, pie chart",
        "Create bar chart showing sales by region",
        "Visualize trends over time",
        "Show distribution of categories"
      ],
      "required_parameters": {
        "data_source": "File path or result from previous step",
        "chart_type": "Type: 'bar', 'line', 'pie', 'scatter'",
        "x_column": "Column name for X-axis (use exact name from file details)",
        "y_column": "Column name for Y-axis (use exact name from file details)"
      },
      "optional_parameters": {
        "title": "Chart title",
        "group_by": "Column to group by"
      },
      "prompt_requirements": [
        "MUST use exact column names from file details",
        "MUST specify chart type",
        "MUST specify X and Y axis columns",
        "Usually placed LAST in workflow",
        "Example: 'Create bar chart from D0_KHC_UK_Beans.arrow with Region on X-axis and Revenue on Y-axis'"
      ],
      "output": "Chart visualization",
      "typical_next_atoms": []
    },
    {
      "atom_id": "feature-overview",
      "name": "Data Overview",
      "description": "Generates comprehensive data overview and statistics",
      "endpoint": "/trinityai/feature-overview",
      "backend_endpoint": "/api/feature-overview/generate",
      "capabilities": [
        "Generate data overview",
        "Show statistics and summaries",
        "Data profiling",
        "Column analysis"
      ],
      "use_cases": [
        "User says: overview, summary, describe, explore data, profile",
        "Get overview of merged data",
        "Understand data structure",
        "See data statistics"
      ],
      "required_parameters": {
        "data_source": "File path or result from previous step"
      },
      "optional_parameters": {},
      "prompt_requirements": [
        "MUST specify exact file path or reference previous step",
        "Example: 'Generate overview of merged_data from previous step'"
      ],
      "output": "Data overview and statistics",
      "typical_next_atoms": ["correlation", "dataframe-operations", "chart-maker", "groupby-wtg-avg"]
    },
    {
      "atom_id": "correlation",
      "name": "Correlation Analysis",
      "description": "Comprehensive correlation analysis for EDA: calculates correlation matrices, identifies relationships between numeric variables, filters data before correlation, finds highest correlation pairs, and analyzes time series correlations",
      "endpoint": "/trinityai/correlation",
      "backend_endpoint": "/api/correlation/calculate",
      "capabilities": [
        "Calculate correlation matrix between numeric columns",
        "Analyze relationships and dependencies between variables",
        "Pearson and Spearman correlation methods",
        "Filter data before correlation analysis",
        "Find highest correlation pairs",
        "Time series correlation analysis",
        "Identify identifiers (categorical) vs measures (numeric) columns",
        "Data exploration and EDA (Exploratory Data Analysis)",
        "Discover patterns and relationships in data",
        "Analyze how variables relate to each other"
      ],
      "use_cases": [
        "User says: correlation, correlate, relationship, dependency, associate, related, connection, link, relationship analysis, EDA, exploratory data analysis, find relationships, which variables are related, how are columns related",
        "Analyze correlation between price and sales",
        "Find relationships between variables in a dataset",
        "Discover which columns are most correlated",
        "Perform EDA to understand data relationships",
        "Identify highly correlated variables",
        "Analyze time series correlations",
        "Understand dependencies between measures",
        "Explore data to find patterns and relationships",
        "Get insights about variable relationships for better analysis"
      ],
      "required_parameters": {
        "data_source": "File path or result from previous step (must have numeric columns)"
      },
      "optional_parameters": {
        "columns": "Specific numeric columns to analyze (if not specified, analyzes all numeric columns)",
        "method": "Correlation method: 'pearson' (linear relationships) or 'spearman' (monotonic relationships)",
        "identifier_filters": "Filter by categorical columns before correlation",
        "measure_filters": "Filter by numeric columns before correlation"
      },
      "prompt_requirements": [
        "MUST specify exact file path or reference previous step",
        "MUST mention correlation, relationship, or EDA analysis",
        "CAN specify specific columns to analyze (optional - will analyze all numeric columns if not specified)",
        "CAN request filtering before correlation (e.g., 'correlate sales and price for North region')",
        "CAN request highest correlation pairs (e.g., 'find the most correlated columns')",
        "Example: 'Calculate correlation matrix for D0_KHC_UK_Beans.arrow'",
        "Example: 'Analyze correlations between all numeric columns in merged_data.arrow'",
        "Example: 'Find which columns are most correlated in sales.arrow'",
        "Example: 'Perform EDA to find relationships between Revenue, Price, and Quantity columns'",
        "Example: 'Analyze correlation between Price and Sales for North region only'"
      ],
      "output": "Correlation matrix with correlation coefficients between numeric columns, can include filtered results and highest correlation pairs",
      "typical_next_atoms": ["chart-maker", "feature-overview", "dataframe-operations", "explore", "groupby-wtg-avg"],
      "eda_workflow_position": "Early to mid workflow - use after data loading/merging but before detailed analysis",
      "best_practices": [
        "Use correlation early in EDA to understand variable relationships",
        "Filter data before correlation if user wants specific subset analysis",
        "Use correlation results to guide further analysis (e.g., highly correlated variables)",
        "Combine with chart-maker to visualize correlation matrices",
        "Use feature-overview before correlation to understand data structure"
      ]
    },
    {
      "atom_id": "explore",
      "name": "Data Exploration",
      "description": "Detailed data exploration and analysis",
      "endpoint": "/trinityai/explore",
      "backend_endpoint": "/api/explore/analyze",
      "capabilities": [
        "Explore data structure",
        "Analyze patterns",
        "Data investigation"
      ],
      "use_cases": [
        "User says: explore, analyze, investigate",
        "Explore merged dataset",
        "Investigate data patterns"
      ],
      "required_parameters": {
        "data_source": "File path or result from previous step"
      },
      "optional_parameters": {},
      "prompt_requirements": [
        "MUST specify exact file path or reference previous step",
        "Example: 'Explore D0_KHC_UK_Beans.arrow'"
      ],
      "output": "Exploration results",
      "typical_next_atoms": ["correlation", "dataframe-operations", "chart-maker", "feature-overview"]
    },
    {
      "atom_id": "data-upload-validate",
      "name": "Load Data",
      "description": "Loads and validates data files (CSV, Excel, Arrow)",
      "endpoint": "/trinityai/upload",
      "backend_endpoint": "/api/data-upload-validate/upload",
      "capabilities": [
        "Upload CSV, Excel, Arrow files",
        "Validate data structure",
        "Schema inference",
        "Data quality checks"
      ],
      "use_cases": [
        "User says: load, upload, import, read",
        "Files don't exist yet and need to be uploaded",
        "First step when working with new files"
      ],
      "required_parameters": {
        "file_path": "Path to file to upload"
      },
      "optional_parameters": {
        "file_type": "File type: 'csv', 'excel', 'arrow'"
      },
      "prompt_requirements": [
        "MUST include exact file path",
        "SKIP this step if files already exist in available_files",
        "Example: 'Upload sales.csv file'"
      ],
      "output": "Validated file ready for use",
      "typical_next_atoms": ["correlation", "feature-overview", "merge", "concat", "dataframe-operations", "groupby-wtg-avg", "chart-maker"]
    }
  ],
  "workflow_rules": [
    "Put data loading (data-upload-validate) FIRST only if files don't exist",
    "Put data transformations (merge, concat, filter, groupby, dataframe-operations) BEFORE visualization",
    "Put chart-maker or visualization atoms LAST",
    "For EDA workflows: Use correlation, feature-overview, or explore early (after data loading/merging) to understand data relationships",
    "Correlation analysis is excellent for EDA - use it when user wants to understand relationships, perform exploratory analysis, or find how variables relate",
    "Each step should build on previous steps",
    "For dataframe-operations: Use ONE atom per task for clarity",
    "Workflows can be long (5-10+ steps) - break complex tasks into individual steps",
    "Consider data dependencies between steps",
    "Use exact file names and column names from file details",
    "Make prompts DETAILED and SPECIFIC (not generic)"
  ],
  "prompt_quality_requirements": [
    "MUST include exact file paths/names (case-sensitive, with extensions)",
    "MUST use exact column names from file details (case-sensitive, including spaces)",
    "MUST include specific values/conditions (not generic)",
    "MUST reference actual data types (numeric vs categorical)",
    "MUST use unique values from categorical columns for filters",
    "BAD: 'Filter data'",
    "GOOD: 'Filter D0_KHC_UK_Beans.arrow where Region = \"North\" AND Revenue > 1000'"
  ]
}

